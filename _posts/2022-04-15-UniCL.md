---
title: UniCL
tags: multi-modal
author: okr
---

(CVPR 22) Unified Contrastive Learning in Image-Text-Label Space

<!--more-->

---

## 0. Abstract

Visual recognition은 지도학습에서는 이미지와 라벨로 구성된 데이터셋, contrastive learning에서는 크롤링한 이미지, 텍스트로 학습해서 feature들을 서로 분리되도록 학습합니다. 본 논문에서는 두가지 데이터 정보인 이미지와 라벨, 이미지와 텍스트 정보를 이용한 하나의 프레임워크를 제안합니다. 

## 1. Introduction   
이전까지 사용하던 이미지와 라벨 데이터는 수집하는데 시간이 오래걸리고 비용이 많이드는 단점이 있고, 크롤링한 이미지와 텍스트 데이터 쌍은 노이즈가 많고 텍스트의 형태가 자유로운 반면 많은 시각정보를 포함합니다(특징을 구분하여 학습하는 능력이 부족).   
따라서 본 논문에서는 **두가지 정보를 하나로 통합하는 공간을 형성해서 이전보다 더 좋은 학습공간을 형성**하고자 합니다. 
<center><img width="465" alt="스크린샷 2022-04-15 오전 1 20 19" src="https://user-images.githubusercontent.com/55619678/163431629-dddaba37-bc31-4219-9485-6f8380e13eb2.png"></center>    

**Contribution**

- 이미지, 텍스트, 라벨 사이의 새로운 공간을 형성해 하나의 공간에 균일하게 맵핑
- 이미지와 라벨, 이미지와 텍스트, 또는 두가지 모두의 contrastive learning을 이용한 학습방법 제시
- 다양한 실험으로 여러 zero-shot 모델과의 비교에서 더 좋은 성능을 보임


## 3. Method   
### 3.1 Preliminaries
본 논문에서는 제시한 세가지의 데이터 타입에 대한 하나의 데이터를 구축합니다.   
$$S= \{(x_n, t_n, y_n)\}^N_{n=1}\;\;\;x(image),\; t(language \;description),\; y(label)$$    
위와 같은 기본적인 구조에서 라벨이 없는 데이터에는 라벨을 연속적으로 없는 숫자를 붙여주고, 텍스트가 없는 데이터에는 라벨에 해당하는 텍스트를 사용합니다.   
<center><img width="472" alt="스크린샷 2022-04-15 오전 1 32 58" src="https://user-images.githubusercontent.com/55619678/163433619-65bc6bb7-47b6-46bf-8323-610512181e6a.png"></center>   

### 3.2 Unified Image-Text-Label Constrast    


$$min\;L_{BiC} = L_{i2t} + L_{t2i}$$    


$$L_{i2t} = -\sum\limits_{i\in\beta}{1\over{|\mathcal{P(i)|}}}\sum\limits_{k\in{\mathcal{P(i)}}}log{exp(\tau u_i^Tv_k)\over{\sum\limits_{j\in\beta}exp(\tau u_i^Tv_j)}}$$   


$$L_{t2i} = -\sum\limits_{j\in\beta}{1\over{|\mathcal{P(j)|}}}\sum\limits_{k\in{\mathcal{P(j)}}}log{exp(\tau u_k^Tv_j)\over{\sum\limits_{j\in\beta}exp(\tau u_i^Tv_j)}}$$    
    

$$\tau\; :\; hyper\; parameter\; for\; controling\; the\; strengths\; of\; penalties\; on\; hard-negative\; samples$$     


위의 그림과 수식을 살펴보면 두번째 수식은 그림에서 image-text-label triplets에서 각 행을 의미하고 세번째 수식은 각 열에 대한 유사도를 의미합니다.   
### 3.3 Discussions & Properties   
위의 $L_{t2i}$는 3가지의 조건을 만족하면 Cross Entropy와 같은 수식이 됩니다. 첫번째 조건은 text encoder가 linear embedding을 하게 하는 것이고, 두번째 조건은 배치크기가 클래스의 개수보다 훨씬 많을 경우, 세번째 조건은 $\tau=1$ 이고, feature에서 l2-normaliation을 사용하지 않고 그대로 사용한다면 그 식은 아래와 같이 Cross Entropy와 같아집니다.   


$$min\;L_{CE} = \sum\limits_{j\in\beta}log{exp(w_{\hat{y}}\widetilde{v}_j+b_{\hat{y}})\over{\sum\limits_{k=1}^K}exp(w_k\widetilde{v}_j+b_k)}$$   


이렇게 cross entropy와 일치한다는 것을 보이는 이유는 이 논문에서 제시한 $L_{BiC}$가 더 강건하다고 말하기 위함입니다. $L_{t2i}$가 cross entropy와 같다면 $L_{i2t}$는 regularizer로서의 역할을 하게 되고 같은 text에 대해서 다른 이미지들이 하나의 text와 유사해져 오버피팅이 되는것을 방지하도록 해줍니다.   
또 다른 특성으로는 CLIP과의 비교가 논문에 서술되어 있는데 **CLIP은 이미지와 텍스트가 1대1 매핑이 되는 것이지만 본 논문에서는 꼭 그렇지 않다는 것을 강조합니다.** 이를 통해서 좀더 다양하게 학습할 수 있다고 여길 수 있습니다.   
<center><img width="472" alt="스크린샷 2022-04-15 오전 2 30 21" src="https://user-images.githubusercontent.com/55619678/163442298-761b88fa-ed6a-48f8-ba2e-27d66cc7cea9.png"></center>    

## 4. Experiments   
<center><img width="983" alt="스크린샷 2022-04-15 오전 2 32 55" src="https://user-images.githubusercontent.com/55619678/163442693-8955ec1c-a8cf-4971-925a-44ed7d436a15.png"></center>      


위의 표를 살펴보면 cifar-10, cifar-100 더 좋은 성능을 보이지만, ImageNet에 대해서는 SupCon(Supervised Constrastive Learning)이 더 좋은 성능을 보임을 알 수 있습니다. 이렇게 두 방법의 성능이 유사하지만 이 논문에서 제시한 UniCL가 갖는 장점은 SupCon에서는 두단계로 linear classifier를 학습시키는 과정이 추가되는 반면 end-to-end로 학습이 가능하다는 것과, language와 함께 멀티모달로 학습해 zero-shot이 가능하다는 점이 있습니다.    

<center><img width="440" alt="스크린샷 2022-04-15 오전 2 38 06" src="https://user-images.githubusercontent.com/55619678/163443597-00102363-1e7e-436c-ac32-8feef37a6561.png"></center>      


또한 위의 표를 통해서 $L_{i2t}$를 사용했을때 더 좋은 성능을 보임을 알 수 있습니다.       
Transformer : Swin Transformer, Embedding : ResNet

<center><img width="477" alt="스크린샷 2022-04-15 오전 2 40 33" src="https://user-images.githubusercontent.com/55619678/163443949-7818e978-ddb1-40c0-9f99-afd1d57b61e5.png"></center>      


또한 위의 Figure를 통해서 더 많은 데이터를 사용할 경우 내용과 가까운 클래스의 물체들이 근처에 모임을 알 수 있습니다.    

 <center><img width="882" alt="스크린샷 2022-04-15 오전 2 43 19" src="https://user-images.githubusercontent.com/55619678/163444565-415f94df-baee-448b-8052-e52fbcdf9db7.png"></center>      


위의 표에서는 zero-shot 성능을 보여 주었는데 CLIP을 사용한경우, Multi-task(Cross Entropy(image-label data)+CLIP(image-text data))을 사용한 경우 보다도 더 좋은 성능을 보이고, 두가지의 다른 구성인 데이터셋을 가지고 학습했을 경우 더 좋은 성능을 보임을 알 수 있습니다.    
또한 YFCC-14M, ImageNet-21k로 학습한 모델을 통해서 YFCC-14M과 달리 ImageNet-21k는 image-label로 이루어져 있는데 visual semantic한 데이터 셋과 견주었을때 비슷한 성능을 보일 수 있게 많은 정보를 줄 수 있다고 설명하였습니다.    

## 5. Conclusion   
본 논문에서는 UniCL이라는 contrastive learning을 사용하는 하나의 멀티 모달 프레임워크를 제안하였습니다. **Image-text-label의 공간을 형성하여 기존의 방법보다 더 좋은 특징 표현 능력을 보여주었습니다.** 
